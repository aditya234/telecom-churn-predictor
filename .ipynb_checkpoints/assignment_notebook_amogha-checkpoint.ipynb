{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Churn Study\n",
    "\n",
    "The objective of this study is to analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n",
    "\n",
    "This study is only to focused on the High value customers(HVCs), which constitutes the top 30 percent of high spending customers\n",
    "\n",
    "we are to build two different types of models:\n",
    "- The predictive model, ie. the one having more accuracy and less bias. For this we'll be using techniques like PCA for dimensionality reduction.\n",
    "- The interpretive model, in which the focus would be on identifying the features which could give us the principle features which cause a customer to churn\n",
    "\n",
    "## Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.decomposition import PCA,IncrementalPCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#42a4f5\", \"#c3c7c9\"]\n",
    "STATE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and identifying the characteristics of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('telecom_churn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "We can eliminate the following columns straight away:\n",
    "- last_date_of_month_6\n",
    "- last_date_of_month_7\n",
    "- last_date_of_month_8\n",
    "- last_date_of_month_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8','last_date_of_month_9'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value treatment\n",
    "#### Dorping the rows which are more than 40% empty and the columns which are more than 60% empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(thresh=df.shape[0]*0.6,how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(thresh=df.shape[1]*0.4,how='all',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifing High Value Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoping our sample and keeping only the High valued customers (top 30%) ie. who demonstrated high spending in the initial two months\n",
    "# creating our target value indicator \"churn\", based on business logics \n",
    "df['total_rech_amt_good_phase'] = (df['total_rech_amt_6'] + df['total_rech_amt_7'])/2\n",
    "total_recharge_amount_good_phase_cutoff = df['total_rech_amt_good_phase'].quantile(0.7)\n",
    "df = df[df['total_rech_amt_good_phase'] > total_recharge_amount_good_phase_cutoff]\n",
    "\n",
    "# dropping the derrived column \"total_rech_amt_good_phase\"\n",
    "df = df.drop(['total_rech_amt_good_phase'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derriving a variable for total overall usage in 9th month\n",
    "df['total_overall_usage'] = df['total_og_mou_9'] + df['total_ic_mou_9'] + df['vol_2g_mb_9'] + df['vol_3g_mb_9']\n",
    "df['churn'] = np.where(df['total_overall_usage']== 0, 1, 0)\n",
    "df['churn'].value_counts()\n",
    "\n",
    "# dropping the arbitary/derrived column \"total_overall_usage\"\n",
    "df = df.drop(['total_overall_usage'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per the analysis scope dropping all the variables of the 9th month, \n",
    "# since we have already derrived a chrun variable for the end prediction analysis\n",
    "ninth_month_columns = [col for col in df.columns if ('_9' in col or 'sep_' in col)]\n",
    "df = df.drop(ninth_month_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derriving the rate of churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " rate_of_churn = round((len(df[df.churn == 1]) / len(df)) * 100, ndigits=2)\n",
    "rate_of_churn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ie. 8.51% customers churn in the 9th month (as per the trends)</b>\n",
    "### Eliminating Columns with zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all columns that have unique values, ie. have exactly zero variance\n",
    "zero_variance_columns = []\n",
    "for col in df.columns:\n",
    "    if len(df[col].value_counts()) == 1:\n",
    "        zero_variance_columns.append(col)\n",
    "# Dropping all columns that have unique values, ie. have exactly zero variance\n",
    "df = df.drop(zero_variance_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_stats():\n",
    "    print(\"No. of columns containing null values\")\n",
    "    print(len(df.columns[df.isna().any()]))\n",
    "\n",
    "    print(\"No. of columns not containing null values\")\n",
    "    print(len(df.columns[df.notna().all()]))\n",
    "\n",
    "    print(\"Total no. of columns in the dataframe\")\n",
    "    print(len(df.columns))\n",
    "\n",
    "    # getting the columns that still have null values\n",
    "    nan_cols = [i for i in df.columns if df[i].isnull().any()]\n",
    "    return nan_cols\n",
    "\n",
    "    \n",
    "null_value_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_be_imputed = ['date_of_last_rech']\n",
    "\n",
    "for field in fields_to_be_imputed:\n",
    "    for month in ['6', '7', '8']:\n",
    "        fields_to_be_imputed = field + '_' + month\n",
    "        df[fields_to_be_imputed].fillna(df[fields_to_be_imputed].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "null_value_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of date of last recharge, we should consider day of recharge\n",
    "df['date_of_last_rech_6'] = pd.to_datetime(df.date_of_last_rech_6).dt.day\n",
    "df['date_of_last_rech_7'] = pd.to_datetime(df.date_of_last_rech_7).dt.day\n",
    "df['date_of_last_rech_8'] = pd.to_datetime(df.date_of_last_rech_8).dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing observations with Median column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = df.columns[df.isnull().sum()>0]\n",
    "for col in missing_cols:\n",
    "    df[col].fillna((df[col].median()), inplace=True)\n",
    "\n",
    "null_value_stats()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence all the missing values have been taken care of, therefore we can move towards data analysis\n",
    "\n",
    "# EDA\n",
    "\n",
    "## Searching and treating outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.01, 0.10,.25,.5,.75,.90,.95,.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = [col for col in df.columns if col not in ['churn','mobile_number']]\n",
    "\n",
    "for col in cont_cols:\n",
    "    percentiles = df[col].quantile([0.01,0.99]).values\n",
    "    df[col][df[col] <= percentiles[0]] = percentiles[0]\n",
    "    df[col][df[col] >= percentiles[1]] = percentiles[1]\n",
    "    \n",
    "df.describe(percentiles=[0.01, 0.10,.25,.5,.75,.90,.95,.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating derived colums\n",
    "- Average values of 3 months for each attributes were created to check if that value could address all the months.\n",
    "- Median value of Internet usage (2G + 3G) across months was created to capture the churn rate as it was observed majority of the churn happens when the internet usage pattern shows a decline\n",
    "- The AON variable was used to create tenure buckets. It was observed larger the tenure, lesser was the churn - as customers who are newly acquired to the network churned more as compared to the old customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_usage_median'] = df[['vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8','vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8']].median(axis=1)\n",
    "df['int_usage_median'] = df.int_usage_median.map(lambda x: 1 if x == 0 else 0)\n",
    "df = df.drop(['vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8','vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8'], 1)\n",
    "\n",
    "\n",
    "df['tenure_buck'] = np.round(df['aon']/365,1)\n",
    "bins = [0, 1, 2, 3, 4, 10]\n",
    "df['tenure_buck'] = pd.cut(df['tenure_buck'], bins)\n",
    "df['tenure_buck'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "to_drop\n",
    "\n",
    "df = df.drop(to_drop, axis=1)\n",
    "df.tenure_buck.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy for tenure_buck \n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "df.rename(columns={\"tenure_buck_(1, 2]\": \"tenure_buck_1_to_2\",\"tenure_buck_(2, 3]\":\"tenure_buck_2_to_3\",\n",
    "                   \"tenure_buck_(3, 4]\":\"tenure_buck_3_to_4\",\n",
    "                   \"tenure_buck_(4, 10]\":\"tenure_buck_4_to_10\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After outlier treatment droping column which don't have much variance or zero variance\n",
    "df = df.drop(['og_others_7','og_others_8','spl_ic_mou_6','spl_ic_mou_7','spl_ic_mou_8','aon'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizating the data for patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "\n",
    "labels =\"No Churn\", \"Churn\"\n",
    "plt.suptitle('Information on Churn', fontsize=20)\n",
    "\n",
    "df[\"churn\"].value_counts().plot.pie(explode=[0,0.25], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, \n",
    "                                             labels=labels, fontsize=12, startangle=70)\n",
    "ax[0].set_ylabel('% of Condition of Churn', fontsize=14)\n",
    "\n",
    "\n",
    "sns.barplot(x=\"date_of_last_rech_6\", y=\"churn\", hue=\"churn\", data=df, palette=colors, estimator=lambda x: len(x) / len(df) * 100)\n",
    "ax[1].set(ylabel=\"(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=\"date_of_last_rech_7\", y=\"churn\", hue=\"churn\", data=df, palette=colors, \n",
    "            estimator=lambda x: len(x) / len(df) * 100)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=\"date_of_last_rech_8\", y=\"churn\", hue=\"churn\", data=df, palette=colors, \n",
    "            estimator=lambda x: len(x) / len(df) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df = df\n",
    "telecom_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Therefore after Exploratory Data Analysis, there are total 67 columns/characteristics left. We'll be now refining our intution further by modeling<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "### Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting feature variable to X\n",
    "X = telecom_df.drop(['churn','mobile_number'],axis=1)\n",
    "\n",
    "# Putting response variable to y\n",
    "y = telecom_df.churn\n",
    "\n",
    "# defining a normalisation function \n",
    "def normalize (x): \n",
    "    return ( (x-np.min(x))/ (max(x) - min(x)))\n",
    "                                                                                          \n",
    "# normalizing all columns \n",
    "X_norm = X.apply(normalize) \n",
    "X_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, train_size=0.7,test_size=0.3,random_state=100)\n",
    "\n",
    "print(f\"Number transactions X_train dataset: {X_train.shape}\")\n",
    "print(f\"Number transactions y_train dataset: {y_train.shape}\")\n",
    "print(f\"Number transactions X_test dataset: {X_test.shape}\")\n",
    "print(f\"Number transactions y_test dataset: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='randomized', random_state=STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the PCA on the train data\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = list(X_train.columns)\n",
    "# pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\n",
    "# pcs_df.head(10)\n",
    "pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], \n",
    "                       'PC3':pca.components_[2],'PC4':pca.components_[3],\n",
    "                       'PC5':pca.components_[4],'PC6':pca.components_[5],\n",
    "                       'PC7':pca.components_[6],'PC8':pca.components_[7],\n",
    "                       'PC9':pca.components_[8],'PC10':pca.components_[9],\n",
    "                       'Feature':colnames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.yticks(np.arange(0, 1.0, 0.1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see 40 component explain more than 90% of variance in the data\n",
    "Hence we consider only those 40 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_final = IncrementalPCA(n_components=40)\n",
    "\n",
    "df_train_pca = pca_final.fit_transform(X_train)\n",
    "df_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating correlation matrix for the principal components we got from pca\n",
    "corrmat = np.corrcoef(df_train_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "sns.heatmap(corrmat,annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving the diagonals which have correlation values = 1\n",
    "# Printing min and max correlation values\n",
    "corrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\n",
    "print(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, after PCA, the dats doesnot has any alarming level of correlation between the principal features\n",
    "\n",
    "#### Therefore transforming components of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pca = pca_final.transform(X_test)\n",
    "df_test_pca.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "plt.scatter(df_train_pca[:,0], df_train_pca[:,1], c = y_train.map({0:'green',1:'red'}))\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our Logistic Regression Model using the above derrived PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_pca = LogisticRegression(class_weight='balanced')\n",
    "model_pca = learner_pca.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n",
    "\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model_pca.predict_proba(df_test_pca)\n",
    "y_pred_default = model_pca.predict(df_test_pca)\n",
    "\n",
    "print(f\"Confusion Matrix  ==> \\n {confusion_matrix(y_test,y_pred_default)}\")\n",
    "print(f\"\\nAccuracy percentage ==> {round(accuracy_score(y_test,y_pred_default) *100, ndigits=2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_df = pd.DataFrame(pred_test)\n",
    "# Converting to column dataframe\n",
    "y_pred_1 = y_pred_df.iloc[:,[1]]\n",
    "\n",
    "# Removing index for both dataframes to append them side by side \n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test,y_pred_1],axis=1)\n",
    "\n",
    "# Renaming the column \n",
    "y_pred_final= y_pred_final.rename(columns={ 1 : 'churn_prob'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds =roc_curve(y_pred_final.churn,y_pred_final.churn_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('ROC_AUC score: ',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_pred_final.churn, y_pred_final.churn_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating accuracy sensitivity and specificity for various probability cutoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_pred_final[i]= y_pred_final.churn_prob.map( lambda x: 1 if x > i else 0)\n",
    "print(f\"{y_pred_final.head()} \\n\\n\")\n",
    "\n",
    "# calculating accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix( y_pred_final.churn, y_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    sensi = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    speci = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.loc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t['threshold'])\n",
    "\n",
    "# Find optimal probability threshold\n",
    "threshold = Find_Optimal_Cutoff(y_pred_final.churn,y_pred_final.churn_prob)\n",
    "print('Threshold: ',threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new column for predicted churn value with 1 if Churn_Prob>0.49 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column 'predicted' with 1 if Churn_Prob>0.49 else 0\n",
    "y_pred_final['pred_churn'] = y_pred_final.churn_prob.map( lambda x: 1 if x > 0.49 else 0)\n",
    "\n",
    "y_pred_final.churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "confusion_mat = metrics.confusion_matrix( y_pred_final.churn, y_pred_final.pred_churn )\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Logistic Regression Model Stats, when PCA is used for dimensionalility reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion_mat[1,1] # true positive \n",
    "TN = confusion_mat[0,0] # true negatives\n",
    "FP = confusion_mat[0,1] # false positives\n",
    "FN = confusion_mat[1,0] # false negatives\n",
    "\n",
    "print('Accuracy Score on test data: ', accuracy_score(y_test,y_pred_default))\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print('Sensitivity: ', TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print('Specificity: ',TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print('false postive rate: ',FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print('positive predictive value: ', TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print('Negative predictive value: ',TN / float(TN+ FN))\n",
    "\n",
    "## Misclassification rate\n",
    "\n",
    "print('Misclassification Rate: ',(FN+FP)/(TP+TN+FP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model have a good level of predictive capibilities, but due to PCA, we can't identify the actual features which have a primary role in churning of a telecom customer in 9th month. \n",
    "#### Therefore to identify the actual features which play a significant role in telecom churn, we are now going to create a model withour PCA. This time, we'll be doing Recusrive Feature Elimination(RFE) for selecting our principle components\n",
    "\n",
    "## LogistcRegression model with RFE\n",
    "\n",
    "We will be selecting top 10 features using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(logreg, 10)\n",
    "rfe = rfe.fit(X_norm,y)\n",
    "\n",
    "#Top 10 features according to RFE\n",
    "col = X_train.columns[rfe.support_]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for calculating VIF\n",
    "def vif_cal(input_data, dependent_col):\n",
    "    vif_df = pd.DataFrame( columns = ['Var', 'Vif'])\n",
    "    x_vars=input_data.drop([dependent_col], axis=1)\n",
    "    xvar_names=x_vars.columns\n",
    "    for i in range(0,xvar_names.shape[0]):\n",
    "        y=x_vars[xvar_names[i]] \n",
    "        x=x_vars[xvar_names.drop(xvar_names[i])]\n",
    "        rsq=sm.OLS(y,x).fit().rsquared  \n",
    "        vif=round(1/(1-rsq),2)\n",
    "        vif_df.loc[i] = [xvar_names[i], vif]\n",
    "    return vif_df.sort_values(by = 'Vif', axis=0, ascending=False, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correlation matrix \n",
    "plt.figure(figsize = (20,10))        # Size of the figure\n",
    "sns.heatmap(X_norm[col].corr(),annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VIF values\n",
    "def print_vifs(columns):\n",
    "    col_for_vif = list(columns)\n",
    "    col_for_vif.append('churn')\n",
    "    return vif_cal(input_data=telecom_df[col_for_vif], dependent_col='churn')\n",
    "\n",
    "print_vifs(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col)\n",
    "# removing arpu_7 due to high VIF, and then looking into vifs again \n",
    "col = col.drop(['arpu_7'])\n",
    "print(col)\n",
    "print_vifs(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing arpu_6 due to high VIF, and then looking into vifs again \n",
    "col = col.drop(['arpu_6'])\n",
    "print(col)\n",
    "print_vifs(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the model using the selected variables\n",
    "logsk = LogisticRegression(class_weight='balanced')\n",
    "logsk.fit(X_train[col], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making prediction on the test data\n",
    "pred_probs_test = logsk.predict_proba(X_test[col])[:,1]\n",
    "\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making prediction on the test data\n",
    "pred_test = logsk.predict_proba(X_test[col])\n",
    "y_pred_default = logsk.predict(X_test[col])\n",
    "\n",
    "print(classification_report(y_test,y_pred_default))\n",
    "print(confusion_matrix(y_test,y_pred_default))\n",
    "print('accuracy_score : ',accuracy_score(y_test,y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_df = pd.DataFrame(pred_test)\n",
    "# Converting to column dataframe\n",
    "y_pred_1 = y_pred_df.iloc[:,[1]]\n",
    "\n",
    "# Removing index for both dataframes to append them side by side \n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test,y_pred_1],axis=1)\n",
    "\n",
    "# Renaming the column \n",
    "y_pred_final= y_pred_final.rename(columns={ 1 : 'churn_prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds =roc_curve(y_pred_final.churn,y_pred_final.churn_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f'ROC_AUC Score: {roc_auc}')\n",
    "draw_roc(y_pred_final.churn, y_pred_final.churn_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_pred_final[i]= y_pred_final.churn_prob.map( lambda x: 1 if x > i else 0)\n",
    "print(f\"{y_pred_final.head()} \\n\\n\")\n",
    "\n",
    "# calculating accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix( y_pred_final.churn, y_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    sensi = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    speci = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal probability threshold\n",
    "threshold = Find_Optimal_Cutoff(y_pred_final.churn,y_pred_final.churn_prob)\n",
    "print('CutOff threshold: ', threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new column for predicted churn value with 1 if Churn_Prob>0.49 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final['pred_churn'] = y_pred_final.churn_prob.map( lambda x: 1 if x > 0.49 else 0)\n",
    "\n",
    "y_pred_final.churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "confusion_mat = metrics.confusion_matrix( y_pred_final.churn, y_pred_final.pred_churn )\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Logistic Regression Model Stats, when RFE is used for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion_mat[1,1] # true positive \n",
    "TN = confusion_mat[0,0] # true negatives\n",
    "FP = confusion_mat[0,1] # false positives\n",
    "FN = confusion_mat[1,0] # false negatives\n",
    "\n",
    "print('Accuracy Score : ',accuracy_score(y_test,y_pred_default))\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print('Sensitivity: ', TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print('Specificity: ',TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print('false postive rate: ',FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print('positive predictive value: ', TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print('Negative predictive value: ',TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above Logestic Regression Model gives good accuracy with PCA and RFE\n",
    "Following are the stats, for a birds eye view:\n",
    "\n",
    "#### WIth PCA\n",
    "- Accuracy Score on test data:  0.8095184644741273\n",
    "- Sensitivity:  0.8154681139755766\n",
    "- Specificity:  0.8022515907978464\n",
    "- false postive rate:  0.1977484092021537\n",
    "- positive predictive value:  0.27108705457825893\n",
    "- Negative predictive value:  0.9796772265391512\n",
    "- Misclassification Rate:  0.1966550679088562\n",
    "\n",
    "#### WIth RFE\n",
    "\n",
    "- Accuracy Score :  0.7786508025592098\n",
    "- Sensitivity: 0.8439620081411127\n",
    "- Specificity: 0.7613803230543319\n",
    "- false postive rate:  0.23861967694566813\n",
    "- positive predictive value:  0.24183514774494558\n",
    "- Negative predictive value: 0.9818526116458892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buliding the model using Random Forest with PCA\n",
    "\n",
    "##### NOTE:\n",
    "We already did do our data treatment using PCA and itentified the principle features, while we were building our Logistic Regression Model. We'll be using the same data to train our Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(df_train_pca,y_train)\n",
    "\n",
    "# Make predictions\n",
    "prediction_test = model_rf.predict(df_test_pca)\n",
    "print ('Randon Forest Accuracy with Default Hyperparameter',metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion_matrix(y_test, prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal n_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(2, 25, 5)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",\n",
    "                  return_train_score=True)\n",
    "rf.fit(df_train_pca,y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting accuracies with min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = rf.best_estimator_\n",
    "rf_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the estimators for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GridSearchCV to find the n_estimators\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'n_estimators': range(100, 1200, 400)}\n",
    "\n",
    "# instantiate the model\n",
    "#here we are mentioninig the maximimum required depth\n",
    "rf = RandomForestClassifier(max_depth=10,class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",\n",
    "                  return_train_score=True)\n",
    "rf.fit(df_train_pca,y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing accuracies between train and test data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "\n",
    "# plotting accuracies with n_estimators\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV to find the min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_leaf': range(100, 400, 50)}\n",
    "\n",
    "# instantiate the rf model\n",
    "rf = RandomForestClassifier(criterion = \"gini\",class_weight='balanced',random_state = 100)\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",\n",
    "                  return_train_score=True)\n",
    "rf.fit(df_train_pca,y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "\n",
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearchCV to find the min_samples_split\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_split': range(100, 400, 50)}\n",
    "\n",
    "# instantiate the model\n",
    "#here we are mentioninig the maximimum required depth\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",\n",
    "                   return_train_score=True\n",
    "               )\n",
    "rf.fit(df_train_pca,y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracies with min_samples_split\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [8,10],\n",
    "    'min_samples_leaf': range(100, 200, 100),\n",
    "    'min_samples_split': range(200, 400, 100),\n",
    "    'n_estimators': range(200, 400, 100),\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,refit='recall_score' ,\n",
    "                          cv = 5, n_jobs=-1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                             max_depth=10,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=200,\n",
    "                             n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred_default = rfc.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred_default))\n",
    "print(confusion_matrix(y_test,y_pred_default))\n",
    "print('accuracy_score:  ',accuracy_score(y_test,y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "confusion_rf_hyper=confusion_matrix(y_test,y_pred_default)\n",
    "confusion_rf_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = confusion_rf_hyper[0,0] # true positive \n",
    "TP = confusion_rf_hyper[1,1] # true negatives\n",
    "FP = confusion_rf_hyper[0,1] # false positives\n",
    "FN = confusion_rf_hyper[1,0] # false negatives\n",
    "\n",
    "print('Accuracy Score:  ',accuracy_score(y_test,y_pred_default))\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print('Sensitivity: ', TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print('Specificity: ',TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print('false postive rate: ',FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print('positive predictive value: ', TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print('Negative predictive value: ',TN / float(TN+ FN))\n",
    "\n",
    "## Misclassification rate\n",
    "\n",
    "print('Misclassification Rate: ',(FN+FP)/(TP+TN+FP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Recomendation for the Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have built Logestic Reression and Random forest models.\n",
    "\n",
    "The insights from the models are :\n",
    "\n",
    "- Logestic Regression is the best model for interpretation with highest sensitivity of 83%(with RFE) and 81%(with PCA)\n",
    "- Random forest is the best model for prediction with highest accuracy of 84%\n",
    "\n",
    "\n",
    "#### Logestic Regression \n",
    "1)With PCA\n",
    "- Accuracy Score-0.8095184644741273\n",
    "- Sensitivity Score-0.8154681139755766\n",
    "\n",
    "2)With RFE\n",
    "- Accuracy Score-0.8145695364238411\n",
    "- Sensitivity Score-0.8371777476255088\n",
    "\n",
    "#### Random Forest\n",
    "1)With PCA\n",
    "- Accuracy Score-0.8495903019418566\n",
    "-  Sensitivity Score-0.7069199457259159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building  Random Forest classifier on alll the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier from sklearn library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing classification report and confusion matrix from sklearn metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "confusion_rf_with_all_feature=confusion_matrix(y_test,predictions)\n",
    "confusion_rf_with_all_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = confusion_rf_with_all_feature[0,0] # true positive \n",
    "TP = confusion_rf_with_all_feature[1,1] # true negatives\n",
    "FP = confusion_rf_with_all_feature[0,1] # false positives\n",
    "FN = confusion_rf_with_all_feature[1,0] # false negatives\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(y_test,predictions))\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print('Sensitivity: ', TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print('Specificity: ',TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print('false postive rate: ',FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print('positive predictive value: ', TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print('Negative predictive value: ',TN / float(TN+ FN))\n",
    "\n",
    "## Misclassification rate\n",
    "\n",
    "print('Misclassification Rate: ',(FN+FP)/(TP+TN+FP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal n_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(2, 20, 5)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\", return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "# plotting accuracies with max_depth\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal n_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'n_estimators': range(100, 1000, 400)}\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "rf = RandomForestClassifier(max_depth=10, class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",return_train_score=True)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "# plotting accuracies with n_estimators\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal min_samples_leaf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_leaf': range(100, 400, 50)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",return_train_score=True)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal min_samples_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_split': range(100, 700, 50)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",return_train_score=True)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rf.cv_results_\n",
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [10,12],\n",
    "    'min_samples_leaf': range(150, 250, 50),\n",
    "    'min_samples_split': range(100, 700, 50),\n",
    "    'n_estimators': [300,400,500]\n",
    "  \n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,refit='recall_score' ,\n",
    "                          cv = 3, verbose = 1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_final = RandomForestClassifier(bootstrap=True,class_weight='balanced',\n",
    "                             max_depth=10,\n",
    "                             min_samples_leaf=150, \n",
    "                             min_samples_split=300,\n",
    "                             n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc_final.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = rfc_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "confusion_rm_f = metrics.confusion_matrix( y_test, predictions )\n",
    "confusion_rm_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = confusion_rm_f[0,0] # true positive \n",
    "TP = confusion_rm_f[1,1] # true negatives\n",
    "FP = confusion_rm_f[0,1] # false positives\n",
    "FN = confusion_rm_f[1,0] # false negatives\n",
    "\n",
    "print('Accuracy Score: ',accuracy_score(y_test,predictions))\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print('Sensitivity: ', TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print('Specificity: ',TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print('false postive rate: ',FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print('positive predictive value: ', TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print('Negative predictive value: ',TN / float(TN+ FN))\n",
    "\n",
    "## Misclassification rate\n",
    "\n",
    "print('Misclassification Rate: ',(FN+FP)/(TP+TN+FP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model with All feature gives us 88% of Accuracy and 80% of sensitivity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Imortant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding the important features\n",
    "importances =rfc_final.feature_importances_\n",
    "col_names =  X.columns\n",
    "\n",
    "#sorting the feautures in descending order to get top features\n",
    "sorted_feature_importance = pd.DataFrame(sorted(zip(importances, list(col_names)), reverse=True),columns={'colName','value'})\n",
    "sorted_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the top 30 features from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 3\n",
    "sorted_feature_importance[0:30].plot(x='colName', y='value' , kind='bar', title='Random Forest Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.xlabel('Top 30 Feature Name')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Top 5 feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1['churn'] = df1['churn'].astype('object', copy = False)\n",
    "df_sample = df1.groupby(['churn'])['arpu_6', 'arpu_7', 'arpu_8'].median()\n",
    "\n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize=(15, 9))\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('Average recharge per month', fontsize=20)\n",
    "\n",
    "    \n",
    "df_sample = df1.groupby(['churn'])['max_rech_amt_6', 'max_rech_amt_7', 'max_rech_amt_8'].mean()\n",
    " \n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize=(15, 9))\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('Maximum Recharge Amount', fontsize=20)\n",
    "    \n",
    "df_sample = df1.groupby(['churn'])['roam_ic_mou_6', 'roam_ic_mou_7', 'roam_ic_mou_8'].mean()\n",
    "\n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize=(15, 9))\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('Average Incoming Call for August - Roaming trend across months', fontsize=20)\n",
    "\n",
    "df_sample = df1.groupby(['churn'])['spl_og_mou_6', 'spl_og_mou_7', 'spl_og_mou_8'].median()\n",
    "\n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize=(15, 9))\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('Special Outgoing Call trend across months', fontsize=20)\n",
    "\n",
    "df_sample = df1.groupby(['churn'])['last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8'].median()\n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize=(15, 9))\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('Distribution of Last Day Recharge Amount', fontsize=20)\n",
    "plt.xlabel(\"Churn\")\n",
    "plt.ylabel(\"Median Last Day Recharge Amount\")\n",
    "\n",
    "df_sample = df1.groupby(['churn'])['ic_others_6', 'ic_others_7', 'ic_others_8'].mean()\n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize=(15, 9))\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('In coming call to Other trend across months', fontsize=20)\n",
    "plt.xlabel(\"Churn\")\n",
    "plt.ylabel(\"Average In coming call to Other\")\n",
    "\n",
    "\n",
    "\n",
    "df_sample = df1.groupby(['churn'])['std_ic_t2f_mou_6', 'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8'].mean()\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('STD incomming  call to fixed trend across months', fontsize=20)\n",
    "plt.xlabel(\"Churn\")\n",
    "plt.ylabel(\"Average STD In coming call to fixed\")\n",
    "\n",
    "\n",
    "df_sample = df1.groupby(['churn'])['aug_vbc_3g', 'jun_vbc_3g', 'jul_vbc_3g'].mean()\n",
    "df_sample.plot.bar()\n",
    "plt.suptitle('Average Volume based cost for 3G', fontsize=20)\n",
    "plt.xlabel(\"Churn\")\n",
    "plt.ylabel(\"Average Volume based cost for 3G\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
